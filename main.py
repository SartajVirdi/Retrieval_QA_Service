# -*- coding: utf-8 -*-
"""Submission3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ND15T64wUPgooHvEAceqkt_JgYGWt50P
"""

# main.py (Optimized for performance)

from flask import Flask, request, jsonify
import cohere
import fitz  # PyMuPDF
import faiss
import numpy as np
import os
import re
import pickle

app = Flask(__name__)

# âœ… Load API key
co = cohere.Client(os.getenv("COHERE_API_KEY"))

# ======================
# STEP 1: Preprocessing (Only ONCE)
# ======================
def load_pdf(path):
    doc = fitz.open(path)
    return "\n".join(page.get_text() for page in doc)

def split_text(text, max_tokens=300):
    sentences = re.split(r'(?<=[.!?])\s+', text)
    chunks, chunk = [], ""
    for sentence in sentences:
        if len(chunk) + len(sentence) <= max_tokens:
            chunk += " " + sentence
        else:
            chunks.append(chunk.strip())
            chunk = sentence
    if chunk:
        chunks.append(chunk.strip())
    return chunks

def embed_chunks(chunks):
    embed_response = co.embed(
        texts=chunks,
        model="embed-english-v3.0",
        input_type="search_document"
    )
    embeddings = np.array(embed_response.embeddings).astype("float32")
    return embeddings

def preprocess_policy_pdf(pdf_path="policy.pdf"):
    print("[INFO] Preprocessing PDF...")
    text = load_pdf(pdf_path)
    chunks = split_text(text)
    embeddings = embed_chunks(chunks)
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)

    with open("chunks.pkl", "wb") as f:
        pickle.dump(chunks, f)
    np.save("embeddings.npy", embeddings)
    faiss.write_index(index, "faiss.index")
    print("[INFO] Preprocessing complete.")

if not os.path.exists("faiss.index"):
    preprocess_policy_pdf()

# ======================
# STEP 2: Webhook Route (Fast)
# ======================
with open("chunks.pkl", "rb") as f:
    chunks = pickle.load(f)
embeddings = np.load("embeddings.npy")
index = faiss.read_index("faiss.index")

def search(query, k=3):
    query_embed = co.embed(
        texts=[query],
        model="embed-english-v3.0",
        input_type="search_query"
    ).embeddings[0]
    D, I = index.search(np.array([query_embed], dtype="float32"), k)
    return [chunks[i] for i in I[0]]

@app.route("/webhook", methods=["POST"])
def webhook():
    data = request.get_json()
    if not data or "query" not in data:
        return jsonify({"error": "Query field missing"}), 400

    query = data["query"]
    retrieved = search(query)
    context = "\n".join(retrieved)

    response = co.chat(
        model='command-r-plus',
        message=query,
        documents=[{"text": context}]
    )

    return jsonify({"response": response.text.strip()})

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)